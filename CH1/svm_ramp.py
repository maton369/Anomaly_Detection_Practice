import numpy as np
from numpy import linalg as LA
import matplotlib.pyplot as plt
import time

# 乱数シードを固定して再現性を確保
np.random.seed(111)

# メインスクリプトとして実行されたときに開始時刻を記録する処理
# （この後のコードでは start は使っていないので、測定用の名残と考えられる）
if __name__ == "__main__":
    start = time.time()

# サンプル数
n = 40

# x: 3×n 行列（2 次元特徴量 + バイアス用 1 の行）
# y: ラベルベクトル（+1 / -1）
x, y = np.zeros((3, n)), np.zeros(n)

# 0〜19 番目のサンプルの x 座標（1 行目）を平均 -15 付近のガウス分布から生成
x[0, 0:20] = np.random.randn(n // 2) - 15

# 20〜39 番目のサンプルの x 座標を平均 -5 付近から生成
x[0, 20::] = np.random.randn(n // 2) - 5

# 2 行目（y 座標）は全サンプルについてガウス分布から生成
x[1, :] = np.random.randn(n)

# 0〜19 番目のラベルを +1、20〜39 番目を -1 に設定（2 クラス問題）
y[0:20] = np.ones(n // 2)
y[20::] = -np.ones(n // 2)

# 最初の 2 点だけ x 座標を +50 シフト
# → 本来左クラスタに属する +1 の点が、右側に大きくはみ出す「強い外れ値」として振る舞う
x[0, 0:2] = x[0, 0:2] + 50

# 3 行目を全て 1 にすることで、バイアス項を明示的に含んだ拡張特徴ベクトルを作る
# つまり各サンプルは (x, y, 1)^T という 3 次元ベクトルとして扱われる
x[2, ::] = 1

# l: 正則化係数（パラメータノルムへのペナルティの強さ）
# e: 重み更新時に使う閾値パラメータ（ロバスト重み付けのスケール）
l, e = 0.0001, 0.001

# t0: パラメータベクトル θ = (w_x, w_y, b)^T の初期値（3×1 ベクトル）
t0 = np.zeros((3, 1))

# 反復回数の上限を 1000 回に設定した IRLS 風の更新ループ
for i in range(1000):
    # m は 1×n ベクトルで、各サンプルについて y_i * (θ^T x_i) を表す
    # （ラベル付きのマージン値：大きいほど分類に自信があり、1 未満だとマージン違反）
    m = np.dot(t0.T, x) * y

    # ここで v を導出するために「1 - m」を 0〜1 の範囲にクリップして足し戻している
    # 内側の max は max(0, 1 - m)、外側の min はその値を高々 1 に制限している
    # v = m + clip(1 - m, 0, 1) という形になっており、以下のような値を取る：
    #   m <= 0   のとき: v = m + 1        （大きく誤分類している点）
    #   0 < m < 1 のとき: v = 1            （マージン内だが正しい側にいる点）
    #   m >= 1   のとき: v = m             （十分マージン外にある点）
    v = m + np.min([np.ones_like(m), np.max([np.zeros_like(m), 1 - m], axis=0)], axis=0)

    # a は |v - m| で、実質的には上述の clip(1 - m, 0, 1) に等しい量
    # 「マージン違反の大きさ」を 1 までに抑えたものと解釈できる
    a = np.abs(v - m)

    # w は各サンプルに対する重みベクトル
    # 初期値は全サンプル 1 としておき、後でロバスト性を考慮した重み付けに更新する
    w = np.ones_like(y)

    # 各サンプルごとにロバスト重みを計算
    # a_j が大きい（マージン違反が大きい）点ほど重みを小さくし、外れ値の影響を抑える
    for j in range(len(w)):
        # a_j > e のとき w_j = e / a_j とする
        # → a_j が大きいほど w_j は小さくなり、大きく誤っている点の寄与が抑制される
        if a[0, j] > e:
            w[j] = e / a[0, j]

    # w を (n,1) の列ベクトルに reshape
    w = np.reshape(w, [40, 1])

    # ここからは「重み付き最小二乗問題」の正規方程式に対応する行列を構成している
    #
    # np.repeat(w, 3, axis=1): 各サンプルの重み w_j を 3 次元分（特徴の次元数分）に複製した (n,3) 行列
    # .T * x: それを転置して (3,n) にし、x (3,n) と要素ごとに積を取ることで
    #         各サンプル x_j に重み w_j を掛けた形にしている
    # 最終的に x · (weighted_x)^T は Σ_j w_j x_j x_j^T に相当する 3×3 行列になる
    #
    # 本来は (X W X^T + λI) の逆行列を取りたいところだが、
    # ここでは (X W X^T)^{-1} + λI という形になっており、
    # 近似的な正則化付き逆行列とみなせる実装になっている
    t1 = LA.inv(np.dot(x, (np.repeat(w, 3, axis=1).T * x).T)) + l * np.eye(3)

    # t2 は右辺ベクトルで、Σ_j w_j v_j y_j x_j を計算している
    # v_j は前段で定義した「クリップされたマージン値」であり、
    # これを y_j と掛けることで、ラベル方向を考慮した目的のターゲット値を作っている
    t2 = np.dot(x, (w.T * v * y).T)

    # t = t1 · t2 は重み付き最小二乗問題の解に対応するパラメータベクトル
    # 反復ごとにロバスト重み w とターゲット v を更新しつつ、
    # IRLS（Iteratively Reweighted Least Squares）的に θ を更新していると解釈できる
    t = np.dot(t1, t2)

    # パラメータの変化量のノルムが十分小さくなったら収束とみなしてループを抜ける
    if LA.norm(t - t0) < 0.1:
        break

    # 次の反復のために現在の推定値を保持
    t0 = t

# 学習されたパラメータ θ = (w_x, w_y, b)^T を出力
print(t)

# 可視化のため、正クラス・負クラスそれぞれの y 座標の最小値・最大値を取得
x1_min = np.amin(x[1, 0:20])
x1_max = np.amax(x[1, 20::])

# 決定境界を描画する x 範囲（横軸の範囲）
z = [-16, 5]

# 正クラス (+1) の点を丸印、負クラス (-1) の点をバツ印でプロット
plt.scatter(x[0, 0:20], x[1, 0:20], marker="o", c=y[0:20])
plt.scatter(x[0, 20::], x[1, 20::], marker="x", c=y[20::])

# 学習したパラメータ t を用いて決定境界（直線）を描画
# t[0] x + t[1] y + t[2] = 0 を y について解くと y = -(t[2] + t[0] x) / t[1] となる
plt.plot(z, -(t[2] + z * t[0]) / t[1], "k-")

# y 軸の表示範囲を少し広げる
plt.ylim([x1_min - 2, x1_max + 2])

# プロットを表示
plt.show()
